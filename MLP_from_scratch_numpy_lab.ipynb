{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1244926",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Lab: Implementing a Multi-Layer Perceptron (MLP) from Scratch using NumPy\n",
    "\n",
    "**Target audience:** 2nd Year B.Tech (Deep Learning Lab)  \n",
    "**Goal:** Build a working MLP (2 → 4 → 1) **without** TensorFlow/PyTorch—only **NumPy**.\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- Forward propagation (prediction)\n",
    "- Activation functions (ReLU, Sigmoid)\n",
    "- Binary Cross-Entropy loss\n",
    "- Backpropagation (gradients)\n",
    "- Gradient Descent updates\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52e376",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Prerequisites (Quick Recap)\n",
    "\n",
    "A neuron computes:\n",
    "\n",
    "\\[ z = XW + b \\]\n",
    "\n",
    "Then applies an activation:\n",
    "\n",
    "\\[ a = f(z) \\]\n",
    "\n",
    "An **MLP** stacks multiple such layers:\n",
    "\n",
    "\\[ X \\rightarrow (W_1,b_1) \\rightarrow a_1 \\rightarrow (W_2,b_2) \\rightarrow \\hat{y} \\]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf49a0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dataset (Simple, small, and visual)\n",
    "\n",
    "We'll start with a tiny binary classification dataset:  \n",
    "Output is 1 **only** when both inputs are 1 (AND-like).\n",
    "\n",
    "This is small enough to debug easily and perfect for learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs (4 samples, 2 features)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=float)\n",
    "\n",
    "# Labels (4 samples, 1 output)\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]], dtype=float)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"\\nX:\\n\", X)\n",
    "print(\"\\ny:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bd554",
   "metadata": {},
   "source": [
    "\n",
    "### Visual intuition (optional)\n",
    "\n",
    "Points with label 1 are the \"positive\" class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b77a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "for i in range(len(X)):\n",
    "    plt.scatter(X[i,0], X[i,1], s=200)\n",
    "    plt.text(X[i,0]+0.02, X[i,1]+0.02, f\"y={int(y[i,0])}\", fontsize=12)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Toy Dataset (AND-like)\")\n",
    "plt.xlim(-0.2, 1.2)\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369b8c8",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Define the MLP Architecture\n",
    "\n",
    "We will build this network:\n",
    "\n",
    "- **Input layer:** 2 features  \n",
    "- **Hidden layer:** 4 neurons (ReLU)  \n",
    "- **Output layer:** 1 neuron (Sigmoid)  \n",
    "\n",
    "So the shapes are:\n",
    "- \\(W_1\\): (2, 4) and \\(b_1\\): (1, 4)  \n",
    "- \\(W_2\\): (4, 1) and \\(b_2\\): (1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d036fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Layer 1: 2 -> 4\n",
    "W1 = np.random.randn(2, 4) * 0.5\n",
    "b1 = np.zeros((1, 4))\n",
    "\n",
    "# Layer 2: 4 -> 1\n",
    "W2 = np.random.randn(4, 1) * 0.5\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "print(\"W1:\", W1.shape, \"b1:\", b1.shape)\n",
    "print(\"W2:\", W2.shape, \"b2:\", b2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8210f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Activation Functions\n",
    "\n",
    "- **ReLU** for hidden layer: \\(\\max(0, z)\\)  \n",
    "- **Sigmoid** for output (probability): \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\n",
    "\n",
    "We also need derivatives for backprop:\n",
    "- ReLU': 1 if z>0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df64913",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Loss Function (Binary Cross-Entropy)\n",
    "\n",
    "For binary classification:\n",
    "\\[\n",
    "L = -\\frac{1}{N} \\sum (y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))\n",
    "\\]\n",
    "\n",
    "We add a tiny value (epsilon) to avoid log(0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred, eps=1e-8):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d0d74",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Forward Propagation (Prediction)\n",
    "\n",
    "Steps:\n",
    "1. Hidden layer linear: \\(z_1 = XW_1 + b_1\\)\n",
    "2. Hidden activation: \\(a_1 = ReLU(z_1)\\)\n",
    "3. Output linear: \\(z_2 = a_1W_2 + b_2\\)\n",
    "4. Output activation: \\(\\hat{y} = sigmoid(z_2)\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    # Hidden layer\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    # Output layer\n",
    "    z2 = a1 @ W2 + b2\n",
    "    y_hat = sigmoid(z2)\n",
    "    \n",
    "    cache = {\"z1\": z1, \"a1\": a1, \"z2\": z2, \"y_hat\": y_hat}\n",
    "    return y_hat, cache\n",
    "\n",
    "y_hat, cache = forward_pass(X, W1, b1, W2, b2)\n",
    "print(\"Initial predictions (y_hat):\\n\", y_hat)\n",
    "print(\"Initial loss:\", binary_cross_entropy(y, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cec24",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Backpropagation (Gradients)\n",
    "\n",
    "We compute gradients for:\n",
    "- \\(W_2, b_2\\) using output error\n",
    "- then propagate error to hidden layer to compute \\(W_1, b_1\\)\n",
    "\n",
    "For sigmoid + BCE, a helpful simplification is:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_2} = \\hat{y} - y\n",
    "\\]\n",
    "\n",
    "Then:\n",
    "- \\(dW_2 = a_1^T dz_2\\)\n",
    "- \\(db_2 = \\sum dz_2\\)\n",
    "- propagate: \\(da_1 = dz_2 W_2^T\\)\n",
    "- apply ReLU derivative: \\(dz_1 = da_1 \\odot ReLU'(z_1)\\)\n",
    "- \\(dW_1 = X^T dz_1\\)\n",
    "- \\(db_1 = \\sum dz_1\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0793a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, y, cache, W2):\n",
    "    z1, a1, y_hat = cache[\"z1\"], cache[\"a1\"], cache[\"y_hat\"]\n",
    "    \n",
    "    # Output layer gradient\n",
    "    dz2 = y_hat - y                        # (N,1)\n",
    "    dW2 = a1.T @ dz2                       # (4,1)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)  # (1,1)\n",
    "    \n",
    "    # Hidden layer gradient\n",
    "    da1 = dz2 @ W2.T                       # (N,4)\n",
    "    dz1 = da1 * relu_derivative(z1)        # (N,4)\n",
    "    dW1 = X.T @ dz1                        # (2,4)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)  # (1,4)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "grads = backward_pass(X, y, cache, W2)\n",
    "for k,v in grads.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68cb5ca",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Training Loop (Gradient Descent)\n",
    "\n",
    "We repeatedly:\n",
    "1. Forward pass\n",
    "2. Compute loss\n",
    "3. Backprop gradients\n",
    "4. Update parameters\n",
    "\n",
    "Watch the loss go down!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(X, y, hidden_size=4, lr=0.1, epochs=5000, seed=42, print_every=500):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Initialize\n",
    "    W1 = np.random.randn(X.shape[1], hidden_size) * 0.5\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, 1) * 0.5\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Forward\n",
    "        y_hat, cache = forward_pass(X, W1, b1, W2, b2)\n",
    "        loss = binary_cross_entropy(y, y_hat)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward\n",
    "        grads = backward_pass(X, y, cache, W2)\n",
    "        \n",
    "        # Update\n",
    "        W1 -= lr * grads[\"dW1\"]\n",
    "        b1 -= lr * grads[\"db1\"]\n",
    "        W2 -= lr * grads[\"dW2\"]\n",
    "        b2 -= lr * grads[\"db2\"]\n",
    "        \n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {loss:.4f}\")\n",
    "    \n",
    "    params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return params, losses\n",
    "\n",
    "params, losses = train_mlp(X, y, hidden_size=4, lr=0.1, epochs=5000, print_every=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2cadf",
   "metadata": {},
   "source": [
    "\n",
    "### Plot Loss Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41608dba",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Evaluate the Model\n",
    "\n",
    "We will print final predictions and convert them to class labels using threshold 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
    "\n",
    "y_hat, _ = forward_pass(X, W1, b1, W2, b2)\n",
    "pred = (y_hat >= 0.5).astype(int)\n",
    "\n",
    "print(\"Final y_hat probabilities:\\n\", y_hat)\n",
    "print(\"\\nPredicted labels:\\n\", pred)\n",
    "print(\"\\nTrue labels:\\n\", y.astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09915f0",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Student Exercises (Do in Lab)\n",
    "\n",
    "1. Change hidden neurons from 4 → 2 → 8. What happens to loss?\n",
    "2. Set learning rate to 1.0. What happens? Why?\n",
    "3. Replace ReLU with **tanh** in the hidden layer:\n",
    "   - Implement `tanh` and its derivative\n",
    "4. Try XOR dataset (more challenging):\n",
    "   - X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "   - y = [[0],[1],[1],[0]]\n",
    "   - Can your network learn it? (Hint: hidden layer helps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57c98b",
   "metadata": {},
   "source": [
    "\n",
    "## 10. (Optional) XOR Challenge Starter Code\n",
    "\n",
    "Uncomment and try after you finish the main part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XOR dataset (uncomment to try)\n",
    "# X_xor = np.array([[0,0],\n",
    "#                   [0,1],\n",
    "#                   [1,0],\n",
    "#                   [1,1]], dtype=float)\n",
    "# y_xor = np.array([[0],\n",
    "#                   [1],\n",
    "#                   [1],\n",
    "#                   [0]], dtype=float)\n",
    "#\n",
    "# params_xor, losses_xor = train_mlp(X_xor, y_xor, hidden_size=4, lr=0.1, epochs=8000, print_every=800)\n",
    "# W1, b1, W2, b2 = params_xor[\"W1\"], params_xor[\"b1\"], params_xor[\"W2\"], params_xor[\"b2\"]\n",
    "# y_hat_xor, _ = forward_pass(X_xor, W1, b1, W2, b2)\n",
    "# pred_xor = (y_hat_xor >= 0.5).astype(int)\n",
    "# print(\"XOR probabilities:\\n\", y_hat_xor)\n",
    "# print(\"XOR predicted labels:\\n\", pred_xor)\n",
    "# print(\"XOR true labels:\\n\", y_xor.astype(int))\n",
    "#\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(losses_xor)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"XOR Training Loss\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91bdaf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Quick Teaching Tips (for the instructor)\n",
    "\n",
    "- Make students **print shapes** after every major step (it prevents 80% of bugs).\n",
    "- Explain: **forward = predict, backward = correct**\n",
    "- Ask them: “What happens if activation is removed?” (They’ll *see* why non-linearity matters.)\n",
    "\n",
    "✅ End of Notebook\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
